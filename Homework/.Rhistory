library(tm)
library(tidyverse)
library(ggthemes)
library(ggrepel)
# 0. Preprocessing - I adapted this from your code
preprocess <- function(text) {
text |>
mutate(
text   = str_to_lower(text),                # turn all letters to lowercase
text   = str_replace_all(text, "\n", " "),  # replace '/n' carriage return symbols
text   = str_remove_all(text, "[:punct:]"), # remove all punctuation
man    = str_count(text, "\\Wman "),        # Basic regex (more about it later in the course. '\\W' part means at the begging of the word) and count those up
by     = str_count(text, "\\Wby "),         # same
upon   = str_count(text, "\\Wupon "),
animosity   = str_count(text, "\\Wanimosity "),
nation   = str_count(text, "\\Wnation "),
people   = str_count(text, "\\Wpeople "),
law   = str_count(text, "\\Wlaw "),
government   = str_count(text, "\\Wgovernment ")) |>
rowwise() |>
mutate(
length = length(str_split(text, "")[[1]])
)
}
federalist_clean <- preprocess(federalist)
# 1.Identify disputed papers
federalist_disputed <- federalist_clean[federalist$author == "HAMILTON OR MADISON",]
federalist_hamilton <-federalist_clean[federalist$author =="HAMILTON",]
federalist_madison <- federalist_clean[federalist$author =="MADISON",]
federalist_jay <- federalist_clean[federalist$author =="JAY",]
# 2. Author frequencies
common_words <- c("animosity", "nation", "people", "law", "government", "man", "by", "upon")
#3. Word frequencies
federalist_clean$text
# 0. Preprocessing - I adapted this from your code
preprocess <- function(text$text) {
# 0. Preprocessing - I adapted this from your code
preprocess <- function(text) {
text |>
mutate(
text   = str_to_lower(text),                # turn all letters to lowercase
text   = str_replace_all(text, "\n", " "),  # replace '/n' carriage return symbols
text   = str_remove_all(text, "[:punct:]"), # remove all punctuation
man    = str_count(text, "\\Wman "),        # Basic regex (more about it later in the course. '\\W' part means at the begging of the word) and count those up
by     = str_count(text, "\\Wby "),         # same
upon   = str_count(text, "\\Wupon "),
animosity   = str_count(text, "\\Wanimosity "),
nation   = str_count(text, "\\Wnation "),
people   = str_count(text, "\\Wpeople "),
law   = str_count(text, "\\Wlaw "),
government   = str_count(text, "\\Wgovernment ")) |>
rowwise() |>
mutate(
length = length(str_split(text, "")[[1]])
)
}
federalist_clean <- preprocess(federalist$text)
federalist_clean = tibble(federalist)
View(federalist_clean)
federalist_clean <- preprocess(federalist_clean)
View(federalist_clean)
# 1.Identify disputed papers
federalist_disputed <- federalist_clean[federalist$author == "HAMILTON OR MADISON",] |>
select(man,by,upon)
View(federalist_disputed)
# 1.Identify disputed papers
federalist_disputed <- federalist_clean[federalist$author == "HAMILTON OR MADISON",] |>
select(man,by,upon, animosity, nation, people, law, government)
View(federalist_disputed)
# 1.Identify disputed papers
federalist_disputed <- federalist_clean[federalist$author == "HAMILTON OR MADISON",] |>
select(man,by,upon, animosity, nation, people, law, government)
federalist_madison <- federalist_clean[federalist$author =="MADISON",] |>
select(man,by,upon, animosity, nation, people, law, government)
federalist_jay <- federalist_clean[federalist$author =="JAY",] |>
select(man,by,upon, animosity, nation, people, law, government)
federalist_hamilton <-federalist_clean[federalist$author =="HAMILTON",] |>
select(man,by,upon, animosity, nation, people, law, government)
# 1.Identify disputed papers
federalist_disputed <- federalist_clean[federalist$author == "HAMILTON OR MADISON",] |>
select(man,by,upon, animosity, nation, people, law, government) |>
colSums()
federalist_disputed
federalist_hamilton
# 1.Identify disputed papers
federalist_disputed <- federalist_clean[federalist$author == "HAMILTON OR MADISON",] |>
select(man,by,upon, animosity, nation, people, law, government) |>
colSums()
federalist_hamilton <-federalist_clean[federalist$author =="HAMILTON",] |>
select(man,by,upon, animosity, nation, people, law, government) |>
colSums()
federalist_madison <- federalist_clean[federalist$author =="MADISON",] |>
select(man,by,upon, animosity, nation, people, law, government) |>
colSums()
federalist_jay <- federalist_clean[federalist$author =="JAY",] |>
select(man,by,upon, animosity, nation, people, law, government) |>
colSums()
federalist_hamilton_prob <- federalist_hamilton / sum(federalist_hamilton)
federalist_hamilton_prob
federalist_madison_prob <- federalist_madison / sum(federalist_madison)
federalist_jay_prob <- federalist_jay / sum(federalist_jay)
p_disputed_hamilton <- dmultinom(federalist_disputed,prob = federalist_hamilton_prob)
p_disputed_madison <- dmultinom(federalist_disputed,prob = federalist_madison_prob)
p_disputed_jay <- dmultinom(federalist_disputed,prob = federalist_jay_prob)
p_disputed_hamilton
p_disputed_madison
p_disputed_jay
table(p_disputed_hamilton)
library(tm)
library(tidyverse)
library(ggthemes)
library(ggrepel)
# 0. Preprocessing - I adapted this from your code
preprocess <- function(text) {
text |>
mutate(
text   = str_to_lower(text),                # turn all letters to lowercase
text   = str_replace_all(text, "\n", " "),  # replace '/n' carriage return symbols
text   = str_remove_all(text, "[:punct:]"), # remove all punctuation
man    = str_count(text, "\\Wman "),        # Basic regex (more about it later in the course. '\\W' part means at the begging of the word) and count those up
by     = str_count(text, "\\Wby "),         # same
upon   = str_count(text, "\\Wupon "),
animosity   = str_count(text, "\\Wanimosity "),
nation   = str_count(text, "\\Wnation "),
people   = str_count(text, "\\Wpeople "),
law   = str_count(text, "\\Wlaw "),
government   = str_count(text, "\\Wgovernment ")) |>
rowwise() |>
mutate(
length = length(str_split(text, "")[[1]])
)
}
federalist_clean = tibble(federalist)
federalist_clean <- preprocess(federalist_clean)
# 1.2. Identify disputed papers, author frequencies, word frequencies
federalist_disputed <- federalist_clean[federalist$author == "HAMILTON OR MADISON",] |>
select(man,by,upon, animosity, nation, people, law, government) |>
colSums()
federalist_hamilton <-federalist_clean[federalist$author =="HAMILTON",] |>
select(man,by,upon, animosity, nation, people, law, government) |>
colSums()
federalist_madison <- federalist_clean[federalist$author =="MADISON",] |>
select(man,by,upon, animosity, nation, people, law, government) |>
colSums()
federalist_jay <- federalist_clean[federalist$author =="JAY",] |>
select(man,by,upon, animosity, nation, people, law, government) |>
colSums()
federalist_hamilton_prob <- (federalist_hamilton +1) / (sum(federalist_hamilton) +1)
federalist_madison_prob <- (federalist_madison +1) / (sum(federalist_madison) +1)
federalist_jay_prob <- (federalist_jay +1) / (sum(federalist_jay) +1)
# 3. Multinomial likelihood for disputed docs
p_disputed_hamilton <- dmultinom(federalist_disputed,prob = federalist_hamilton_prob)
p_disputed_madison <- dmultinom(federalist_disputed,prob = federalist_madison_prob)
p_disputed_jay <- dmultinom(federalist_disputed,prob = federalist_jay_prob)
p_disputed_hamilton
p_disputed_madison
p_disputed_jay
federalist_hamilton_prob
p_disputed_hamilton <- dmultinom(federalist_disputed[1],prob = federalist_hamilton_prob)
p_disputed_hamilton <- dmultinom(federalist_disputed[1,],prob = federalist_hamilton_prob)
p_disputed_hamilton <- dmultinom(federalist_disputed[,1],prob = federalist_hamilton_prob)
federalist_disputed
View(federalist_clean)
federalist_disputed <- federalist_clean[federalist$author == "HAMILTON OR MADISON",] |>
select(man,by,upon, animosity, nation, people, law, government)
View(federalist_clean)
federalist_disputed
# Function to calculate likelihoods for each disputed paper
calculate_likelihoods <- function(disputed_docs, prob_hamilton, prob_madison, prob_jay) {
disputed_docs <- as.data.frame(disputed_docs)  # Ensure it's a data frame
likelihoods <- disputed_docs %>%
rowwise() %>%
mutate(
p_hamilton = dmultinom(c_across(everything()), prob = prob_hamilton),
p_madison  = dmultinom(c_across(everything()), prob = prob_madison),
p_jay      = dmultinom(c_across(everything()), prob = prob_jay),
likely_author = case_when(
p_hamilton == max(p_hamilton, p_madison, p_jay) ~ "HAMILTON",
p_madison  == max(p_hamilton, p_madison, p_jay) ~ "MADISON",
p_jay      == max(p_hamilton, p_madison, p_jay) ~ "JAY"
)
)
return(likelihoods)
}
# Apply the function to disputed documents
disputed_likelihoods <- calculate_likelihoods(federalist_disputed,
federalist_hamilton_prob,
federalist_madison_prob,
federalist_jay_prob)
# Check the structure of disputed documents
print(dim(federalist_disputed))  # Should show 11 rows and 8 columns
# Check the length of each author's probability vector
print(length(federalist_hamilton_prob))  # Should be 8
print(length(federalist_madison_prob))   # Should be 8
print(length(federalist_jay_prob))       # Should be 8
calculate_likelihoods <- function(disputed_docs, prob_hamilton, prob_madison, prob_jay) {
disputed_docs <- as.data.frame(disputed_docs)  # Ensure it's a data frame
# Specify the word columns explicitly
word_columns <- c("man", "by", "upon", "animosity", "nation", "people", "law", "government")
likelihoods <- disputed_docs %>%
rowwise() %>%
mutate(
p_hamilton = dmultinom(c_across(all_of(word_columns)), prob = prob_hamilton),
p_madison  = dmultinom(c_across(all_of(word_columns)), prob = prob_madison),
p_jay      = dmultinom(c_across(all_of(word_columns)), prob = prob_jay),
likely_author = case_when(
p_hamilton == max(p_hamilton, p_madison, p_jay) ~ "HAMILTON",
p_madison  == max(p_hamilton, p_madison, p_jay) ~ "MADISON",
p_jay      == max(p_hamilton, p_madison, p_jay) ~ "JAY"
)
)
return(likelihoods)
}
# Apply the function
disputed_likelihoods <- calculate_likelihoods(federalist_disputed,
federalist_hamilton_prob,
federalist_madison_prob,
federalist_jay_prob)
# View the output
print(disputed_likelihoods)
# Vectors for each author (from the training data)
mu_madison <- colSums(federalist_madison)
library(ggplot2)
library(ggthemes)
library(dplyr)
# Select 'by' and 'upon' for visualization
vector_visualizations_by_upon <- vector_visualizations %>%
select(by, upon, author)
# Reshape the data into a wide format where rows are authors and columns are words
library(tidyr)
vector_data <- selected_words_df %>%
pivot_wider(names_from = word, values_from = n, values_fill = 0)
# 3. Multinomial likelihood for each disputed document under each author's distribution
# Function to calculate multinomial likelihoods for all disputed documents
calculate_likelihoods <- function(disputed_docs, author_probs) {
apply(disputed_docs, 1, function(doc) dmultinom(doc, prob = author_probs))
}
# Calculate likelihoods
likelihoods_hamilton <- calculate_likelihoods(federalist_disputed, federalist_hamilton_prob)
likelihoods_madison  <- calculate_likelihoods(federalist_disputed, federalist_madison_prob)
likelihoods_jay      <- calculate_likelihoods(federalist_disputed, federalist_jay_prob)
# Combine likelihoods into a data frame for easier plotting
likelihoods_df <- tibble(
Document = 1:nrow(federalist_disputed),
Hamilton = likelihoods_hamilton,
Madison  = likelihoods_madison,
Jay      = likelihoods_jay
)
# Reshape for plotting
library(tidyr)
likelihoods_long <- likelihoods_df %>%
pivot_longer(cols = -Document, names_to = "Author", values_to = "Likelihood")
# 4. Plotting the results
library(ggplot2)
ggplot(likelihoods_long, aes(x = Document, y = Likelihood, color = Author)) +
geom_line(size = 1) +
geom_point(size = 2) +
scale_y_log10() +  # Use log scale for better visibility if likelihoods are very small
labs(title = "Multinomial Likelihoods of Disputed Federalist Papers",
x = "Disputed Document",
y = "Likelihood (log scale)",
color = "Author") +
theme_minimal()
# 0. Preprocessing - I adapted this from your code
preprocess <- function(text) {
text |>
mutate(
text   = str_to_lower(text),                # turn all letters to lowercase
text   = str_replace_all(text, "\n", " "),  # replace '/n' carriage return symbols
text   = str_remove_all(text, "[:punct:]"), # remove all punctuation
man    = str_count(text, "\\Wman "),        # Basic regex (more about it later in the course. '\\W' part means at the begging of the word) and count those up
by     = str_count(text, "\\Wby "),         # same
upon   = str_count(text, "\\Wupon "),
animosity   = str_count(text, "\\Wanimosity "),
nation   = str_count(text, "\\Wnation "),
people   = str_count(text, "\\Wpeople "),
law   = str_count(text, "\\Wlaw "),
government   = str_count(text, "\\Wgovernment ")) |>
rowwise() |>
mutate(
length = length(str_split(text, "")[[1]])
)
}
federalist_clean = tibble(federalist)
federalist_clean <- preprocess(federalist_clean)
# 1.2. Identify disputed papers, author frequencies, word frequencies
federalist_disputed <- federalist_clean[federalist$author == "HAMILTON OR MADISON",] |>
select(man,by,upon, animosity, nation, people, law, government)
federalist_hamilton <-federalist_clean[federalist$author =="HAMILTON",] |>
select(man,by,upon, animosity, nation, people, law, government) |>
colSums()
federalist_madison <- federalist_clean[federalist$author =="MADISON",] |>
select(man,by,upon, animosity, nation, people, law, government) |>
colSums()
federalist_jay <- federalist_clean[federalist$author =="JAY",] |>
select(man,by,upon, animosity, nation, people, law, government) |>
colSums()
federalist_hamilton_prob <- (federalist_hamilton +1) / (sum(federalist_hamilton) +1)
federalist_madison_prob <- (federalist_madison +1) / (sum(federalist_madison) +1)
federalist_jay_prob <- (federalist_jay +1) / (sum(federalist_jay) +1)
# 3. Multinomial likelihood for disputed docs
p_disputed_hamilton <- dmultinom(federalist_disputed[,1],prob = federalist_hamilton_prob)
rbind(federalist_disputed, federalist_hamilton, federalist_madison, federalist_jay)
federalist_jay
# 1.2. Identify disputed papers, author frequencies, word frequencies
federalist_disputed <- federalist_clean[federalist$author == "HAMILTON OR MADISON",] |>
select(man,by,upon, animosity, nation, people, law, government) |>
colSums()
rbind(federalist_disputed, federalist_hamilton, federalist_madison, federalist_jay)
library(tm)
library(tidyverse)
library(ggthemes)
library(ggrepel)
# 0. Preprocessing - I adapted this from your code
preprocess <- function(text) {
text |>
mutate(
text   = str_to_lower(text),                # turn all letters to lowercase
text   = str_replace_all(text, "\n", " "),  # replace '/n' carriage return symbols
text   = str_remove_all(text, "[:punct:]"), # remove all punctuation
man    = str_count(text, "\\Wman "),        # Basic regex (more about it later in the course. '\\W' part means at the begging of the word) and count those up
by     = str_count(text, "\\Wby "),         # same
upon   = str_count(text, "\\Wupon "),
through   = str_count(text, "\\Wthrough "),
nation   = str_count(text, "\\Wnation "),
people   = str_count(text, "\\Wpeople "),
law   = str_count(text, "\\Wlaw "),
government   = str_count(text, "\\Wgovernment ")) |>
rowwise() |>
mutate(
length = length(str_split(text, "")[[1]])
)
}
federalist_clean = tibble(federalist)
federalist_clean <- preprocess(federalist_clean)
# 1.2. Identify disputed papers, author frequencies, word frequencies
federalist_disputed <- federalist_clean[federalist$author == "HAMILTON OR MADISON",] |>
select(man,by,upon, through, nation, people, law, government) |>
colSums()
federalist_hamilton <-federalist_clean[federalist$author =="HAMILTON",] |>
select(man,by,upon, through, nation, people, law, government) |>
colSums()
federalist_madison <- federalist_clean[federalist$author =="MADISON",] |>
select(man,by,upon, through, nation, people, law, government) |>
colSums()
federalist_jay <- federalist_clean[federalist$author =="JAY",] |>
select(man,by,upon, through, nation, people, law, government) |>
colSums()
federalist_hamilton_prob <- (federalist_hamilton +1) / (sum(federalist_hamilton) +1)
federalist_madison_prob <- (federalist_madison +1) / (sum(federalist_madison) +1)
federalist_jay_prob <- (federalist_jay +1) / (sum(federalist_jay) +1)
# 3. Multinomial likelihood for disputed docs
p_disputed_hamilton <- dmultinom(federalist_disputed[,1],prob = federalist_hamilton_prob)
p_disputed_hamilton <- dmultinom(federalist_disputed,prob = federalist_hamilton_prob)
p_disputed_madison <- dmultinom(federalist_disputed,prob = federalist_madison_prob)
p_disputed_jay <- dmultinom(federalist_disputed,prob = federalist_jay_prob)
p_disputed_hamilton
p_disputed_madison
p_disputed_jay
rbind(federalist_disputed, federalist_hamilton, federalist_madison, federalist_jay)
rbind(p_disputed_hamilton, p_disputed_madison, p_disputed_jay)
federalist_hamilton_prob
p_disputed_hamilton
rbind(p_disputed_hamilton, p_disputed_madison, p_disputed_jay)
federalist_disputed
federalist_clean[federalist$author == "HAMILTON OR MADISON",] |>
select(man,by,upon, through, nation, people, law, government)
# Function to calculate likelihoods for each disputed document
calculate_likelihoods <- function(disputed_docs, author_probs) {
apply(disputed_docs, 1, function(doc) dmultinom(doc, prob = author_probs))
}
# Calculate likelihoods for each author
p_disputed_hamilton <- calculate_likelihoods(federalist_disputed, federalist_hamilton_prob)
# Add likelihood calculations directly to the disputed documents dataframe
federalist_disputed_likelihoods <- federalist_clean |>
filter(author == "HAMILTON OR MADISON") |>  # Filter disputed papers
select(man, by, upon, through, nation, people, law, government) |>
rowwise() |>
mutate(
Hamilton_Likelihood = dmultinom(c_across(man:government), prob = federalist_hamilton_prob),
Madison_Likelihood  = dmultinom(c_across(man:government), prob = federalist_madison_prob),
Jay_Likelihood      = dmultinom(c_across(man:government), prob = federalist_jay_prob),
Most_Likely_Author  = case_when(
Hamilton_Likelihood == max(c(Hamilton_Likelihood, Madison_Likelihood, Jay_Likelihood)) ~ "Hamilton",
Madison_Likelihood  == max(c(Hamilton_Likelihood, Madison_Likelihood, Jay_Likelihood)) ~ "Madison",
Jay_Likelihood      == max(c(Hamilton_Likelihood, Madison_Likelihood, Jay_Likelihood)) ~ "Jay"
)
) |>
ungroup()
# View results
print(federalist_disputed_likelihoods |> select(Hamilton_Likelihood, Madison_Likelihood, Jay_Likelihood, Most_Likely_Author))
federalist_disputed_prob <- (federalist_disputed +1) / (sum(federalist_disputed) +1)
federalist_disputed_prob
p_disputed_hamilton <- dmultinom(federalist_disputed_prob,prob = federalist_hamilton_prob)
p_disputed_hamilton
dmultinom(federalist_disputed_prob,prob = federalist_hamilton_prob)
dmultinom(federalist_disputed_prob,prob = federalist_madison_prob)
# Function to calculate likelihoods for a single document
calculate_likelihoods <- function(document_counts, hamilton_prob, madison_prob, jay_prob) {
hamilton_likelihood <- dmultinom(document_counts, prob = hamilton_prob)
madison_likelihood  <- dmultinom(document_counts, prob = madison_prob)
jay_likelihood      <- dmultinom(document_counts, prob = jay_prob)
return(c(Hamilton = hamilton_likelihood, Madison = madison_likelihood, Jay = jay_likelihood))
}
# Select disputed documents
federalist_disputed_docs <- federalist_clean |>
filter(author == "HAMILTON OR MADISON") |>
select(man, by, upon, through, nation, people, law, government)
# Apply the function to each row
likelihoods_list <- apply(federalist_disputed_docs, 1, function(row) {
calculate_likelihoods(as.numeric(row), federalist_hamilton_prob, federalist_madison_prob, federalist_jay_prob)
})
# Convert the result into a data frame
likelihoods_df <- as_tibble(do.call(rbind, likelihoods_list))
# Apply the function to each row using lapply instead of apply
likelihoods_list <- lapply(1:nrow(federalist_disputed_docs), function(i) {
document_counts <- as.numeric(federalist_disputed_docs[i, ])
calculate_likelihoods(document_counts, federalist_hamilton_prob, federalist_madison_prob, federalist_jay_prob)
})
# Convert the list of likelihoods into a data frame
likelihoods_df <- as_tibble(do.call(rbind, likelihoods_list))
# Add document numbers
likelihoods_df <- likelihoods_df |> mutate(Document = row_number())
# See results
print(likelihoods_df)
calculate_likelihoods <- function(document_counts, hamilton_prob, madison_prob, jay_prob) {
tibble(
Hamilton = dmultinom(document_counts, prob = hamilton_prob, log = TRUE),
Madison = dmultinom(document_counts, prob = madison_prob, log = TRUE),
Jay = dmultinom(document_counts, prob = jay_prob, log = TRUE)
)
}
# Apply the function to each row using lapply instead of apply
likelihoods_list <- lapply(1:nrow(federalist_disputed_docs), function(i) {
document_counts <- as.numeric(federalist_disputed_docs[i, ])
calculate_likelihoods(document_counts, federalist_hamilton_prob, federalist_madison_prob, federalist_jay_prob)
})
# Convert the list of likelihoods into a data frame
likelihoods_df <- as_tibble(do.call(rbind, likelihoods_list))
# Add document numbers
likelihoods_df <- likelihoods_df |> mutate(Document = row_number())
# See results
print(likelihoods_df)
likelihoods_list <- lapply(1:nrow(federalist_disputed_docs), function(i) {
document_counts <- as.numeric(federalist_disputed_docs[i, ])
calculate_likelihoods(document_counts, federalist_hamilton_prob, federalist_madison_prob, federalist_jay_prob)
})
likelihoods_list
library(tm)
library(tidyverse)
library(ggthemes)
library(ggrepel)
# 0. Preprocessing - I adapted this from your code
preprocess <- function(text) {
text |>
mutate(
text   = str_to_lower(text),                # turn all letters to lowercase
text   = str_replace_all(text, "\n", " "),  # replace '/n' carriage return symbols
text   = str_remove_all(text, "[:punct:]"), # remove all punctuation
man    = str_count(text, "\\Wman "),        # Basic regex (more about it later in the course. '\\W' part means at the begging of the word) and count those up
by     = str_count(text, "\\Wby "),         # same
upon   = str_count(text, "\\Wupon "),
through   = str_count(text, "\\Wthrough "),
nation   = str_count(text, "\\Wnation "),
people   = str_count(text, "\\Wpeople "),
law   = str_count(text, "\\Wlaw "),
government   = str_count(text, "\\Wgovernment ")) |>
rowwise() |>
mutate(
length = length(str_split(text, "")[[1]])
)
}
federalist_clean = tibble(federalist)
federalist_clean <- preprocess(federalist_clean)
# 1.2. Identify disputed papers, author frequencies, word frequencies
federalist_disputed <- federalist_clean[federalist$author == "HAMILTON OR MADISON",] |>
select(man,by,upon, through, nation, people, law, government) |>
colSums()
federalist_hamilton <-federalist_clean[federalist$author =="HAMILTON",] |>
select(man,by,upon, through, nation, people, law, government) |>
colSums()
federalist_madison <- federalist_clean[federalist$author =="MADISON",] |>
select(man,by,upon, through, nation, people, law, government) |>
colSums()
federalist_jay <- federalist_clean[federalist$author =="JAY",] |>
select(man,by,upon, through, nation, people, law, government) |>
colSums()
federalist_disputed_prob <- (federalist_disputed +1) / (sum(federalist_disputed) +1)
federalist_hamilton_prob <- (federalist_hamilton +1) / (sum(federalist_hamilton) +1)
federalist_madison_prob <- (federalist_madison +1) / (sum(federalist_madison) +1)
federalist_jay_prob <- (federalist_jay +1) / (sum(federalist_jay) +1)
# 3. Multinomial likelihood for disputed docs
p_disputed_hamilton <- dmultinom(federalist_disputed,prob = federalist_hamilton_prob)
p_disputed_madison <- dmultinom(federalist_disputed,prob = federalist_madison_prob)
p_disputed_jay <- dmultinom(federalist_disputed,prob = federalist_jay_prob)
knitr::opts_chunk$set(echo = TRUE)
clean_federalist <- federalist %>%
mutate(                           # the mutate() function is part of dplyr package / allows to change stuff within the dataframe easily
text   = str_to_lower(text),                # turn all letters to lowercase
text   = str_replace_all(text, "\n", " "),  # replace '\n' carriage return symbols
# text = str_replace_all(text, "  ", " "),
text = trimws(text),                        # remove leading and trailing whitespaces
text   = str_remove_all(text, "[:punct:]"), # remove all punctuation
man    = str_count(text, "\\Wman "),        # Basic regex (more about it later in the course. '\\W' part means at the begging of the word) and count those up
by     = str_count(text, "\\Wby "),         # same
upon   = str_count(text, "\\Wupon ")        # same
) %>%
rowwise() %>%                                 # make future functions work rowwise
mutate(
length = length(str_split(text, " ")[[1]])  # calculate the length of the text (in words)
)
clean_federalist$text[1]
